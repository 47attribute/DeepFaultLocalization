./one_click.sh: line 2: [Closure: command not found
./one_click.sh: line 28: [Closure: command not found
./one_click.sh: line 34: [Closure: command not found
./one_click.sh: line 40: [Closure: command not found
./one_click.sh: line 46: [Closure: command not found
./one_click.sh: line 52: [Closure: command not found
./one_click.sh: line 58: [Closure: command not found
./one_click.sh: line 2: [Lang: command not found
./one_click.sh: line 28: [Lang: command not found
./one_click.sh: line 34: [Lang: command not found
./one_click.sh: line 40: [Lang: command not found
./one_click.sh: line 46: [Lang: command not found
./one_click.sh: line 52: [Lang: command not found
./one_click.sh: line 58: [Lang: command not found
./one_click.sh: line 2: [: missing `]'
./one_click.sh: line 28: [: missing `]'
./one_click.sh: line 34: [: missing `]'
./one_click.sh: line 40: [: missing `]'
./one_click.sh: line 46: [: missing `]'
./one_click.sh: line 52: [: missing `]'
./one_click.sh: line 58: [: missing `]'
./one_click.sh: line 2: [: missing `]'
./one_click.sh: line 28: [: missing `]'
./one_click.sh: line 34: [: missing `]'
./one_click.sh: line 40: [: missing `]'
./one_click.sh: line 46: [: missing `]'
./one_click.sh: line 52: [: missing `]'
./one_click.sh: line 58: [: missing `]'
./one_click.sh: line 2: [all: command not found
./one_click.sh: line 28: [all: command not found
./one_click.sh: line 34: [all: command not found
./one_click.sh: line 40: [all: command not found
./one_click.sh: line 46: [all: command not found
./one_click.sh: line 52: [all: command not found
./one_click.sh: line 58: [all: command not found
./one_click.sh: line 2: [all: command not found
./one_click.sh: line 28: [all: command not found
./one_click.sh: line 34: [all: command not found
./one_click.sh: line 40: [all: command not found
./one_click.sh: line 46: [all: command not found
./one_click.sh: line 52: [all: command not found
./one_click.sh: line 58: [all: command not found
./one_click.sh: line 2: [all: command not found
./one_click.sh: line 28: [all: command not found
./one_click.sh: line 34: [all: command not found
./one_click.sh: line 40: [all: command not found
./one_click.sh: line 46: [all: command not found
./one_click.sh: line 52: [all: command not found
./one_click.sh: line 58: [all: command not found
Closure-{1..133}
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/DeepFL/fc_based.py", line 86, in run
    datasets = input.read_data_sets(trainFile, trainLabelFile, testFile, testLabelFile, groupFile)
  File "/home/weili/DeepFL/input.py", line 86, in read_data_sets
    train_instances=readFile(train_file)
  File "/home/weili/DeepFL/input.py", line 7, in readFile
    with open(filename) as f:
IOError: [Errno 2] No such file or directory: './DeepFL/Closure/{1..133}/Train.csv'
Lang-{1..65}
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/DeepFL/fc_based.py", line 86, in run
    datasets = input.read_data_sets(trainFile, trainLabelFile, testFile, testLabelFile, groupFile)
  File "/home/weili/DeepFL/input.py", line 86, in read_data_sets
    train_instances=readFile(train_file)
  File "/home/weili/DeepFL/input.py", line 7, in readFile
    with open(filename) as f:
IOError: [Errno 2] No such file or directory: './DeepFL/Lang/{1..65}/Train.csv'
Math-{1..106}
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/DeepFL/fc_based.py", line 86, in run
    datasets = input.read_data_sets(trainFile, trainLabelFile, testFile, testLabelFile, groupFile)
  File "/home/weili/DeepFL/input.py", line 86, in read_data_sets
    train_instances=readFile(train_file)
  File "/home/weili/DeepFL/input.py", line 7, in readFile
    with open(filename) as f:
IOError: [Errno 2] No such file or directory: './DeepFL/Math/{1..106}/Train.csv'
./one_click.sh: 4: ./one_click.sh: data: not found
./one_click.sh: 5: [: =: unexpected operator
./one_click.sh: 31: [: =: unexpected operator
./one_click.sh: 37: [: =: unexpected operator
./one_click.sh: 43: [: =: unexpected operator
./one_click.sh: 49: [: =: unexpected operator
./one_click.sh: 55: [: =: unexpected operator
./one_click.sh: 61: [: =: unexpected operator
Math-{1..106}
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/DeepFL/fc_based.py", line 86, in run
    datasets = input.read_data_sets(trainFile, trainLabelFile, testFile, testLabelFile, groupFile)
  File "/home/weili/DeepFL/input.py", line 86, in read_data_sets
    train_instances=readFile(train_file)
  File "/home/weili/DeepFL/input.py", line 7, in readFile
    with open(filename) as f:
IOError: [Errno 2] No such file or directory: './DeepFL/Math/{1..106}/Train.csv'
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[0]=24334: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[1]=24335: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[2]=24336: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[3]=24337: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[4]=24338: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[5]=24340: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[6]=24341: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[7]=24342: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[8]=24343: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[9]=24344: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[10]=24346: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[11]=24347: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[12]=24349: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[13]=24351: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[14]=24352: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[15]=24354: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[16]=24356: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[17]=24360: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[18]=24362: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[19]=24363: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[20]=24368: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[21]=24370: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[22]=24374: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[23]=24378: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[24]=24379: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[25]=24386: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[26]=24390: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[27]=24392: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[28]=24396: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[29]=24398: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[30]=24400: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[31]=24405: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[32]=24409: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[33]=24410: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[34]=24414: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[35]=24417: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[36]=24418: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[37]=24421: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[38]=24425: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[39]=24426: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[40]=24427: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[41]=24428: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[42]=24430: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[43]=24431: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[44]=24432: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[45]=24434: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[46]=24435: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[47]=24436: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[48]=24438: not found
./quick_closure.sh: 30: ./quick_closure.sh: Bad substitution
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[0]=24482: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[1]=24483: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[2]=24484: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[3]=24485: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[4]=24487: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[5]=24488: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[6]=24490: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[7]=24491: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[8]=24492: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[9]=24493: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[10]=24494: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[11]=24495: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[12]=24496: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[13]=24497: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[14]=24498: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[15]=24499: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[16]=24500: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[17]=24501: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[18]=24502: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[19]=24504: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[20]=24505: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[21]=24506: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[22]=24508: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[23]=24509: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[24]=24510: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[25]=24512: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[26]=24513: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[27]=24515: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[28]=24517: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[29]=24518: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[30]=24519: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[31]=24520: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[32]=24521: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[33]=24522: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[34]=24942: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[35]=24971: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[36]=24999: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[37]=25034: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[38]=25060: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[39]=25085: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[40]=25103: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[41]=25115: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[42]=25133: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[43]=25159: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[44]=25180: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[45]=25200: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[46]=25213: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[47]=25214: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[48]=25215: not found
./quick_closure.sh: 30: ./quick_closure.sh: Bad substitution
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[0]=25999: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[1]=26005: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[2]=26006: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[3]=26007: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[4]=26008: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[5]=26009: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[6]=26010: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[7]=26011: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[8]=26012: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[9]=26013: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[10]=26014: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[11]=26015: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[12]=26016: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[13]=26088: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[14]=26089: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[15]=26090: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[16]=26091: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[17]=26092: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[18]=26093: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[19]=26094: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[20]=26095: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[21]=26096: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[22]=26097: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[23]=26098: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[24]=26099: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[25]=26100: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[26]=26102: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[27]=26104: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[28]=26105: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[29]=26106: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[30]=26107: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[31]=26108: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[32]=26109: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[33]=26110: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[34]=26111: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[35]=26112: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[36]=26113: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[37]=26114: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[38]=26115: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[39]=26116: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[40]=26117: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[41]=26565: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[42]=26566: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[43]=26567: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[44]=26568: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[45]=26569: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[46]=26570: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[47]=26571: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[48]=26572: not found
./quick_closure.sh: 30: ./quick_closure.sh: Bad substitution
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[0]=26848: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[1]=26849: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[2]=26850: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[3]=26851: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[4]=26852: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[5]=26853: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[6]=26854: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[7]=26855: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[8]=26856: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[9]=26857: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[10]=26858: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[11]=26859: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[12]=26860: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[13]=26861: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[14]=26862: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[15]=26863: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[16]=26864: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[17]=26865: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[18]=26866: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[19]=26867: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[20]=26868: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[21]=26869: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[22]=26870: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[23]=26871: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[24]=26872: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[25]=26873: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[26]=26874: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[27]=26875: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[28]=26876: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[29]=26877: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[30]=26878: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[31]=26957: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[32]=26958: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[33]=26959: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[34]=26960: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[35]=26961: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[36]=26962: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[37]=26963: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[38]=26964: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[39]=26966: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[40]=26967: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[41]=26968: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[42]=26970: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[43]=26971: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[44]=26972: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[45]=26973: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[46]=27367: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[47]=27369: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[48]=27370: not found
./quick_closure.sh: 30: ./quick_closure.sh: Bad substitution
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[0]=27447: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[1]=27448: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[2]=27449: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[3]=27450: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[4]=27451: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[5]=27452: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[6]=27453: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[7]=27454: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[8]=27455: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[9]=27456: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[10]=27457: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[11]=27458: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[12]=27459: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[13]=27460: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[14]=27461: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[15]=27462: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[16]=27463: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[17]=27464: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[18]=27465: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[19]=27466: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[20]=27467: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[21]=27468: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[22]=27469: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[23]=27470: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[24]=27471: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[25]=27475: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[26]=27476: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[27]=27477: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[28]=27478: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[29]=27479: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[30]=27480: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[31]=27481: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[32]=27482: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[33]=27483: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[34]=27484: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[35]=27485: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[36]=27486: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[37]=27487: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[38]=27488: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[39]=27489: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[40]=27490: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[41]=27491: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[42]=27492: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[43]=27526: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[44]=27540: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[45]=27553: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[46]=27561: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[47]=27562: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[48]=27564: not found
./quick_closure.sh: 30: ./quick_closure.sh: Bad substitution
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[0]=28141: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[1]=28142: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[2]=28143: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[3]=28144: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[4]=28145: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[5]=28146: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[6]=28147: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[7]=28148: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[8]=28149: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[9]=28150: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[10]=28151: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[11]=28152: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[12]=28153: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[13]=28154: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[14]=28155: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[15]=28156: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[16]=28157: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[17]=28158: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[18]=28159: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[19]=28161: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[20]=28163: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[21]=28164: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[22]=28165: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[23]=28166: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[24]=28167: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[25]=28168: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[26]=28169: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[27]=28170: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[28]=28171: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[29]=28172: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[30]=28173: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[31]=28174: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[32]=28175: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[33]=28176: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[34]=28177: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[35]=28178: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[36]=28179: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[37]=28180: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[38]=28181: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[39]=28182: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[40]=28183: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[41]=28184: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[42]=28185: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[43]=28186: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[44]=28187: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[45]=28188: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[46]=28189: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[47]=28190: not found
./quick_closure.sh: 25: ./quick_closure.sh: BACK_PID[48]=28191: not found
./quick_closure.sh: 30: ./quick_closure.sh: Bad substitution
Lang-55
Lang-57
Lang-17
Lang-31
Lang-61
Lang-47
Lang-44
Lang-46
Lang-53
Lang-40
Lang-38
Lang-45
2019-01-18 12:47:21.696817: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:21.729647: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Lang-1
2019-01-18 12:47:21.761324: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:21.795781: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:21.835758: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:21.861931: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:21.880064: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:21.896744: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:21.913974: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:21.946448: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:21.979124: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.014801: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.035990: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.058826: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.095776: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.141124: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.161960: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.197606: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.234297: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.254284: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Lang-58
2019-01-18 12:47:22.286549: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.327345: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.373612: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.416576: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.447493: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.472729: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.511958: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.552909: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.588581: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.610876: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.620460: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.634156: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.645092: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.662953: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.671050: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.681381: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.695856: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Lang-25
2019-01-18 12:47:22.715140: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.731468: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Lang-7
2019-01-18 12:47:22.762773: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.787616: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.813425: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.828991: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.846413: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.853010: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Lang-42
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
2019-01-18 12:47:22.856571: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:22.862606: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Mockito-22
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:22.868438: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.879768: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.899432: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.901263: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:22.917085: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.938994: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.940232: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:22.952709: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.970507: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.974374: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:22.981768: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.995356: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:22.998099: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.003621: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.015455: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.021301: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.026326: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.045264: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.054676: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.062408: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.084037: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.089611: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.093306: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.100584: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.121697: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.127222: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.128019: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.135501: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.160493: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.167553: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.168411: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.175255: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.194488: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.202423: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.214552: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.223319: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.250966: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.255468: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.258917: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.268668: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.288309: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.292421: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.294583: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.304875: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.334837: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.338016: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.349780: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.374500: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.392416: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.421562: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.431307: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.450986: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.457735: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.487313: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.499637: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.519691: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.566856: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.586461: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:23.603129: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.610919: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Chart-13
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:23.618610: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:23.635979: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.655192: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.674774: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.677940: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.719143: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.724917: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:23.725925: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:23.730481: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.765183: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.775296: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.804904: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.809835: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.915276: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.980587: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:23.982747: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:24.013393: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:24.040915: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:24.043463: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:24.065019: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:24.071656: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:24.102833: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:24.104829: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:24.113408: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:24.148624: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Lang-49
2019-01-18 12:47:24.187218: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Lang-3
2019-01-18 12:47:24.537345: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:24.627812: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:24.644677: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Time-2
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:24.682251: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:24.729743: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:24.945884: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:25.016166: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Time-5
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:25.020247: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Math-4
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
Lang-36
2019-01-18 12:47:25.079613: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Lang-41
2019-01-18 12:47:25.260277: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:25.585704: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:25.641794: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:25.655639: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:25.735686: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:25.749611: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Lang-64
2019-01-18 12:47:26.061294: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Lang-54
2019-01-18 12:47:26.133750: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:26.159974: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:26.171222: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:26.193662: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:26.220694: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:26.231195: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Lang-21
Lang-48
2019-01-18 12:47:26.982385: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:26.985435: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:27.057329: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:27.058989: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:27.105598: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:27.109376: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:27.177972: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Lang-34
2019-01-18 12:47:27.718497: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:28.848247: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:28.890550: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:28.938218: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Lang-19
2019-01-18 12:47:29.049607: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Lang-37
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:29.058491: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Chart-16
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
Lang-23
Lang-29
Lang-27
2019-01-18 12:47:31.637703: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:31.685926: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Lang-50
2019-01-18 12:47:31.730367: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:31.778862: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:31.831902: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:32.951943: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:33.014478: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Lang-39
Lang-43
2019-01-18 12:47:33.310131: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Time-16
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:33.361258: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:33.387140: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:33.392652: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Math-37
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:33.423260: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Time-15
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:33.435767: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:33.489678: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:33.514342: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:33.535148: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:33.594199: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Lang-63
2019-01-18 12:47:33.660013: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:33.701225: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:33.706932: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:33.762503: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:33.768194: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:33.836791: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:33.840582: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:33.894335: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:33.894691: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:34.230121: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:34.278633: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:34.330951: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:34.385759: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:34.432755: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:34.478327: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:34.534903: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:34.595303: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:34.673924: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Chart-22
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
Lang-5
2019-01-18 12:47:35.030513: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Chart-25
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:35.144787: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Chart-23
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:35.161392: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Time-8
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:35.442820: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Chart-14
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:35.513931: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Time-27
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:35.717860: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Mockito-25
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
Lang-13
2019-01-18 12:47:36.409581: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Mockito-27
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:36.424297: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Time-13
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
Lang-15
2019-01-18 12:47:36.858397: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Chart-3
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:36.991334: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Time-11
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:37.012578: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Time-4
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:37.062057: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.122945: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.142871: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Chart-12
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:37.198013: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.251762: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.315274: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-19
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:37.320395: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Lang-11
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:37.326967: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.369485: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.383576: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.430039: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.458401: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.478924: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.515368: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.516369: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Chart-2
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:37.544176: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.549401: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Math-29
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:37.580562: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.611596: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.620290: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Mockito-14
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:37.639009: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.676012: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.691219: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Chart-7
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:37.734197: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.776119: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.776372: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Mockito-9
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:37.963447: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:37.987874: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.021764: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.052784: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.109177: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.138514: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.169848: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.184644: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.225318: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.241556: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Chart-4
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:38.246315: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.281982: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.332535: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.351629: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.369503: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.389811: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.402586: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.415049: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.438546: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.474004: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.514510: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.540507: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.559290: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-1
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:38.562085: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.583611: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.584930: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:38.627002: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.642111: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.658571: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.680952: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.698175: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.734228: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.769173: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.783125: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.801094: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.827232: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.842926: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Mockito-10
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:38.846845: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.871451: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.903725: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:38.917087: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.022052: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.038056: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.052797: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Time-7
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:39.066812: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Lang-62
2019-01-18 12:47:39.100093: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.127560: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.168802: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.208544: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.265087: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.265619: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Time-12
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:39.279835: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.304557: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Time-9
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:39.325343: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Chart-9
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:39.420063: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.450815: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.466860: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.561031: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.597560: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.618206: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.640654: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.687826: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.693758: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Chart-6
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:39.699219: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.723512: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.735961: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.764558: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.772755: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.788509: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Lang-35
2019-01-18 12:47:39.796479: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_LAUNCH_FAILED
Lang-52
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:39.804520: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.893118: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.903418: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.914808: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.932365: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:39.981874: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.000860: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.009280: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.030292: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.055585: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.065678: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.093852: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.107281: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.150644: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.159701: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.181425: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.204762: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.228618: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.233866: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.262097: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.271574: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.309453: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.318913: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.335786: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.358903: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.390111: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.397050: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.432174: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.442180: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.488468: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.496480: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.518583: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.526432: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.574928: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.580298: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.602899: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.611946: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.647481: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.654794: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.701866: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.711538: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.766522: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.783967: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.805649: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.807383: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.824948: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.866685: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.878043: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.915278: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.916682: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.923785: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:40.982536: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.003887: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.004787: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.025850: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.078284: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.103189: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.111115: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.120267: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.120659: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Mockito-12
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:41.175855: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.192284: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.196897: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.206143: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.232505: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Math-25
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:41.246451: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.266489: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.284496: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.291781: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.359531: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.379542: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.382743: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.402339: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.458320: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.472705: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.477495: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.485873: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.547687: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.563598: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.581829: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.665590: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Chart-10
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:41.706039: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.713432: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.738854: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.809338: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.811559: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.836673: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:41.967871: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.017889: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.020477: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.040982: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.052696: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.191044: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.203389: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.221755: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.251726: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.325677: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.357653: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.432549: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.452315: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.538016: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.557816: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Time-10
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: 2019-01-18 12:47:42.558726: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Failed to create session.
2019-01-18 12:47:42.646397: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.659594: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.727826: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.742559: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.821559: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.826952: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Lang-60
2019-01-18 12:47:42.915073: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:42.927923: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.029327: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.145036: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.161166: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.259955: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.291248: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.374615: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.391604: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.457855: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.489827: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.509809: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_LAUNCH_FAILED
Math-10
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:43.567076: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.584753: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.650232: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.679008: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.740703: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.768179: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.854731: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.891854: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.959886: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:43.978706: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:44.027151: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-41
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:44.044249: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:44.099230: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:44.163556: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:44.213181: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:44.313006: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:44.464597: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:44.527972: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:44.569285: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:44.610094: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:44.650345: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:44.722502: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:44.784676: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:44.852113: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:44.880677: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:44.960675: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.009383: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.055631: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.093816: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.141554: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Mockito-16
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:45.167318: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.187842: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.263227: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Lang-51
2019-01-18 12:47:45.293050: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.330075: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-43
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:45.332065: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.346956: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Lang-33
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:45.373051: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.456473: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.491742: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.569299: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.587077: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.640938: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.691168: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.708193: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.725983: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.758451: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.820772: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.830139: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.856070: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.897898: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.962224: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.975608: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:45.991525: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.020449: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.095656: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.100381: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.116697: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.145505: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.168658: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.258788: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.259949: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.271252: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.272095: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-45
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:46.281283: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.322775: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.334296: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.335661: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.371683: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.387705: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.398907: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.437428: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.445078: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:46.453942: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.459063: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.489798: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.520235: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.531157: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.533143: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:46.562703: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.569874: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Chart-21
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:46.586668: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:46.590079: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.592134: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.626592: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.665881: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.689115: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.700049: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Time-18
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:46.707166: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:46.722859: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.734669: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:46.736620: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.739026: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.767414: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.779635: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Mockito-32
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:46.794087: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.811615: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.826087: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:46.840376: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.843241: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Math-73
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:46.930282: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.932879: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.943816: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.982907: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:46.986979: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Math-75
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:46.995404: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.002810: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:47.010102: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.037838: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.038600: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.058101: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.101237: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Mockito-33
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:47.111581: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.118591: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.139342: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Lang-9
2019-01-18 12:47:47.182696: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.183682: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.195013: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.222805: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.248975: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.259002: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.296133: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.296916: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.299066: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Math-67
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:47.303734: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.320877: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Time-17
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:47.342346: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.356966: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.363721: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.383113: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.394407: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.397159: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:47.398411: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.404044: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.430794: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.449551: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.460262: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.489465: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.491380: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.502131: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.503989: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:47.509841: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.533919: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.552902: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.563108: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.597001: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.598912: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.600340: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.621901: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.629021: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:47.665941: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.689728: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.707492: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.728898: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.747532: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.748630: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.754944: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.763810: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:47.786213: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.799977: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.810425: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:47.810809: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.861037: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.876999: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.878329: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.881910: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.886639: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:47.902581: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:47.903153: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.915103: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.920440: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.921844: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:47.957070: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.961990: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.962664: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.976789: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:47.998501: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.030409: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.047726: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.051681: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.053686: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.057742: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.060554: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-49
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:48.067879: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Time-19
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:48.088054: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.091288: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.094394: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.099153: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.109675: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.131301: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.143121: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.154680: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.156299: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.164167: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.196753: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-47
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
2019-01-18 12:47:48.197644: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:48.199052: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.207534: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.212131: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.222313: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.253878: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.263487: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.283786: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.285317: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.297386: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.313844: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.314805: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.316981: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.327435: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.489501: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.513927: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.526883: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.547714: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.558608: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.561999: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.587797: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.588721: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.591827: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.607625: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.626156: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.633015: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.655254: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.658244: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.661563: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.689382: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.690534: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.695755: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.716016: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.766532: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.782843: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.792506: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.800456: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.801289: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.834153: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Mockito-18
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:48.836985: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.837709: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.839368: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.858898: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:48.890346: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:48.900518: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.062085: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.074446: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.079977: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.086842: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.106332: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.107181: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.109124: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.126133: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.144004: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.150962: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.163740: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.170009: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.173687: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.184670: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.193867: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.194673: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.198042: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.227653: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.246755: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.252736: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.260022: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.268710: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.279434: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.300549: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.311748: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.312523: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.314798: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.328664: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.346064: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.355161: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.359081: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.364439: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.370789: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.375142: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.383119: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.383811: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.390658: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.415131: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.435980: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.446065: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.450901: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.455520: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.456432: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.462408: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.464117: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.478380: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.478920: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.483385: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.505920: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.525590: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.527974: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.534226: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.61M (1690112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.539190: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.540338: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.550331: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.551540: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.582488: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.590138: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.597192: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.604663: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.633271: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.637060: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.643742: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.45M (1521152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.653000: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.654809: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.656741: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.657039: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.682570: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.683542: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.687489: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.699739: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.716103: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.726887: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.728850: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.734814: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.31M (1369088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.747441: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.752087: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.752707: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.754663: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.765919: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.766575: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.772846: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.792738: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.805658: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.808473: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.811711: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.830386: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.843133: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.844270: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.847682: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.856832: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.857589: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.861651: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.875419: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.898093: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.901981: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.913386: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.949425: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.950361: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.960945: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:49.966367: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.967098: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.976449: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:49.993744: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.011042: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.018335: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.021543: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.049684: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.051459: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.063545: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.065232: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.065604: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.074981: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.100918: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.115531: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.119650: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.129018: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.149970: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.160517: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.162266: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.163477: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.168566: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.170155: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.195723: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.213879: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.217265: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.220655: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.229958: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.250102: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.250776: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.255663: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.259031: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.260486: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.268901: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.289279: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.299820: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.305568: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.311329: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.321452: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.339546: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.340525: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.357173: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.359025: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.361756: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.368304: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.377491: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.383751: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.387280: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.393639: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.422922: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.433362: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.438305: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.446712: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.459643: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.463461: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.472392: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.475675: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.480617: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.484633: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.506451: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.515820: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.518594: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.525869: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.528964: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.554076: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.61M (1690112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.554501: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.561416: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.568729: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.577362: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.590523: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.595118: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.599495: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.609139: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.635414: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.635598: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.45M (1521152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.640071: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.642149: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.666251: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.687274: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.694902: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.696950: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.710298: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.725175: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.738042: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.746419: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.748835: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.770555: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.792718: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.795326: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.800389: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.814421: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.840699: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.847340: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.847828: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.852693: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.864652: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.869567: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Math-85
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:50.884324: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.885330: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.887120: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.891548: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.905497: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.918068: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.925898: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.949902: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:50.952095: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:50.960885: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.147577: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Mockito-37
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:51.152601: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.178202: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.179659: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.179925: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.180626: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.203682: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.217116: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.222064: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.226050: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.227553: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.232830: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.241723: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.251784: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.253791: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.254525: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.255738: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.260767: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.284218: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.297891: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.301820: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.302901: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.304847: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.318275: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.340051: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.346786: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.347557: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.348812: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.351124: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.377545: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.392864: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.395854: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.398072: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.404112: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.405056: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.410335: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Mockito-29
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 15), b.shape=(15, 15), m=500, n=15, k=15
	 [[Node: fc/similar/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_8_0_8/_15, fc/similar/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'fc/similar/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 214, in mutation_spec_first
    similar_1 = single_fc_layer(similarity,15,15*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 15), b.shape=(15, 15), m=500, n=15, k=15
	 [[Node: fc/similar/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_8_0_8/_15, fc/similar/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:47:51.425819: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.435869: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.436625: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.438120: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.459927: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.474205: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.475951: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.494838: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.507227: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.527441: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.534380: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.539168: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.540042: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.541713: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.576865: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.589640: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.592157: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.598311: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.625851: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.638493: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.639263: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.642919: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.645272: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.768770: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.770559: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.783048: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.803141: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.804001: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.806538: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.812986: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.835852: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.838252: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.849837: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.851958: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.880327: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.889964: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.890706: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.892343: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.910064: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.927769: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.929087: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.933005: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.942353: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.962180: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.962728: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:51.982577: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.984168: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:51.987418: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.008530: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.028754: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.029418: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.034825: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.045133: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.048566: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.054098: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Lang-56
2019-01-18 12:47:52.068988: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.070179: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.071063: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.100062: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.107514: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.108184: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.129950: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.135047: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.144277: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.150734: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.172808: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.173807: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.174936: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.235446: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.238736: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.239669: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.267767: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.271032: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.282337: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.286543: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.289722: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.297585: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.298223: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.299245: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.317431: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.335767: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.336945: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.337402: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.339070: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.351387: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.354943: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.359615: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.361290: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.369695: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.371497: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.371923: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.372775: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.394692: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.411980: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.416075: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.420102: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.421949: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.425792: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.429523: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.430617: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.431433: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.448068: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.449324: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.450595: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.453277: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.454068: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.460459: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.488513: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.492627: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.499663: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.507465: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.512491: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.517149: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.519921: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.520682: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.523436: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.535820: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.537569: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.541540: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.542403: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.546177: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.548238: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.563729: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:52.582430: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.584577: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.587263: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.589586: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.599862: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.613819: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.616927: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.617963: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.627461: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.635794: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.637324: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.637795: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.642380: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.643137: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.663195: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:52.671959: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.674698: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.677637: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.687739: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.690735: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.691735: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.702896: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.708088: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.709381: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.712495: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.720797: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.730572: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.751066: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:52.760997: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.761507: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.767569: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.775168: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.775979: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Mockito-21
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:52.780478: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.784010: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.795845: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.798443: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.800483: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.805044: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.809433: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.821669: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:52.828162: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.830490: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.835788: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.846060: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.854093: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.857724: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.867382: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.868793: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.872448: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.881680: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.886217: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.893290: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.898617: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.915069: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:52.921967: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.924614: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.952918: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.955290: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.963107: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.964137: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.964198: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.975556: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.977027: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.980364: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.980960: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:52.987065: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.988695: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:52.998751: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.000592: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.011866: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:53.019658: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.025279: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.036092: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.040959: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.043344: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.045658: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.050396: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.057535: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.066579: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.067430: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.072020: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.077227: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.079833: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.087480: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.095354: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.096780: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:53.105396: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.107286: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.112966: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.116862: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.122193: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.123097: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.124978: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.135468: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.137667: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.148957: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.168726: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.177024: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.180945: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:53.188035: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.188779: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.192216: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.196877: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.200511: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.204141: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.204868: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.205971: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.225872: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.246731: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.259239: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.264014: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.273750: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:53.274480: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.275997: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.276077: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.282891: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.288267: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.292270: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.299155: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.300167: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.302262: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.309851: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.335225: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.344442: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.350131: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.354210: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.362898: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:53.364958: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.368573: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.372949: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.375953: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.386330: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.390469: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.391375: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.394409: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.400456: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.418073: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.441597: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.446810: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.453902: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.457236: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.460953: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.463493: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:53.467702: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.474141: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.474351: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.489481: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.491792: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.493671: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.501784: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.530698: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.534595: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.541147: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.552324: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.558722: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.559527: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.568921: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.570026: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:53.582066: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.587013: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.588968: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.594843: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.595468: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.598796: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.600625: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.611627: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.643107: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.644280: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.645412: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.655327: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.663406: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.667526: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.670839: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:53.674238: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.677646: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.680110: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.684986: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.685613: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.690135: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.694550: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.699453: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.733670: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.734606: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.735811: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.746311: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.750747: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.759727: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.766644: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:53.768847: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.774150: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.775630: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.778478: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.779248: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.788913: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.791936: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.797290: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.840349: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.842416: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.863862: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.865954: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.881441: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.884496: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:53.888933: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.896206: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.898673: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.899563: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.904772: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.908374: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.912730: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.914014: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.918941: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.952108: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.953150: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.961365: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.964114: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Lang-59
2019-01-18 12:47:53.981476: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.984097: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:53.985642: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:53.987781: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.989382: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.990923: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:53.996263: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.001638: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.003636: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.009929: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.030003: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.037739: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.039684: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.048717: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.056630: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:54.059960: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.060268: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.062381: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.065444: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.074362: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.078579: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.61M (1690112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.080297: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.117693: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.120031: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.131359: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.134253: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.146980: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.149814: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:54.152181: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Chart-17
2019-01-18 12:47:54.157845: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.159923: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.173794: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.175381: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.176079: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.179769: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.45M (1521152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.186180: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.207986: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.218668: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.223727: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.226599: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.239976: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.241433: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.243355: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:54.245140: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.251040: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.254676: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.260390: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.31M (1369088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.261888: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.271866: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.272920: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.307079: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.315039: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.320233: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.331066: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.332763: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.337307: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.337438: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.339760: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:54.341547: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.343733: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.348810: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.17M (1232384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.349695: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.356059: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.360377: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.378225: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.387977: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.390244: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.396622: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.409551: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.412426: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.414050: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.415168: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:54.419702: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.421694: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.428685: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.06M (1109248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.430186: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.432993: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.438496: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.451838: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.474638: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.488304: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.492869: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.502682: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.504403: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.505985: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.508791: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:54.510547: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.512367: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.516761: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.522160: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.525249: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.531787: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.540957: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.561288: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.570761: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.571578: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.598864: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.600037: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.603560: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.604789: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:54.605641: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.607147: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.614158: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.623256: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.624416: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.632152: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.635779: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.737783: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.746559: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.746768: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Chart-5
2019-01-18 12:47:54.776284: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.779595: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.782806: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.783084: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:54.784026: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.790372: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.798692: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.799733: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.807607: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.817706: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.829329: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.837230: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.839540: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.864066: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.866393: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.868201: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:54.870184: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.874809: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.877499: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.885172: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.886442: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.905894: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.915428: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.916537: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.932521: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.938788: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.954938: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.962860: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:54.963612: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.964525: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:54.973229: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.976983: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.984508: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:54.987396: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Lang-65
2019-01-18 12:47:55.004192: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.015651: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.040250: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.044240: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.047997: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.058682: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.063403: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.066132: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.066610: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:55.073427: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.075544: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.078279: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.085084: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.086183: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Mockito-34
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut3/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_4_0_5/_7, seperate_mut/mut3/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_mut/mut3/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 193, in mutation_spec_first
    mut_3 = single_fc_layer(m3,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut3/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_4_0_5/_7, seperate_mut/mut3/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:47:55.106041: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.112411: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.120254: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.125314: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.128436: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.146610: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.149359: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.150906: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.156353: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:55.162360: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.163619: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.174602: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.183072: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.200959: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.204214: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.217495: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.224419: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.228032: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.255380: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.258511: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:55.272438: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.273300: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.280727: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.307528: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.309953: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.319594: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.324282: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.334392: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.350517: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.363105: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.370037: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.375521: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-57
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:55.395835: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.403742: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.406710: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.427692: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.434803: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.449777: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.466893: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.468438: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Time-25
2019-01-18 12:47:55.500392: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.503379: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.506999: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.513837: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.524906: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.545561: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.555692: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.570438: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.588323: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.595402: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.601798: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.606376: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.623731: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.626608: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.636858: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.637965: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.651580: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.655221: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.661189: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.668545: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.674568: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.682370: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Mockito-35
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_2_0_3/_1, seperate_mut/mut1/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_mut/mut1/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 187, in mutation_spec_first
    mut_1 = single_fc_layer(m1,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_2_0_3/_1, seperate_mut/mut1/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:47:55.708106: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.711313: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.715699: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Mockito-38
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut2/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_3_0_4/_5, seperate_mut/mut2/weight/read)]]
	 [[Node: fc/similar/weight/Regularizer/l2_regularizer/_35 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1815_fc/similar/weight/Regularizer/l2_regularizer", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_mut/mut2/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 190, in mutation_spec_first
    mut_2 = single_fc_layer(m2,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut2/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_3_0_4/_5, seperate_mut/mut2/weight/read)]]
	 [[Node: fc/similar/weight/Regularizer/l2_regularizer/_35 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1815_fc/similar/weight/Regularizer/l2_regularizer", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:47:55.725745: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.727808: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.745575: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.754831: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.760811: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.766495: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.770672: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.786732: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.790172: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.808802: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.834632: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.837296: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.849095: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.855390: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.61M (1690112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.860277: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.875365: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:55.878430: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:55.892931: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.006370: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.010045: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Math-33
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
2019-01-18 12:47:56.012224: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError2019-01-18 12:47:56.012845: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_LAUNCH_FAILED
Mockito-15
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:56.017299: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.45M (1521152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_2_0_3/_1, seperate_mut/mut1/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_mut/mut1/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 187, in mutation_spec_first
    mut_1 = single_fc_layer(m1,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_2_0_3/_1, seperate_mut/mut1/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:47:56.030238: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.046400: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:56.047336: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.185092: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.198274: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.216724: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.221690: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:56.239408: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.31M (1369088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:56.241907: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.277004: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:56.278435: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.292476: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.296792: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.326001: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.329833: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:56.338035: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.17M (1232384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:56.366011: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Math-83
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError2019-01-18 12:47:56.379081: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut2/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_3_0_4/_5, seperate_mut/mut2/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_mut/mut2/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 190, in mutation_spec_first
    mut_2 = single_fc_layer(m2,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut2/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_3_0_4/_5, seperate_mut/mut2/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:47:56.391821: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.399037: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.405398: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.425405: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Math-87
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError2019-01-18 12:47:56.429433: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_2_0_3/_1, seperate_mut/mut1/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_mut/mut1/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 187, in mutation_spec_first
    mut_1 = single_fc_layer(m1,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_2_0_3/_1, seperate_mut/mut1/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:47:56.432409: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.06M (1109248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:56.539204: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:56.569920: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.586413: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Chart-11
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 15), b.shape=(15, 15), m=500, n=15, k=15
	 [[Node: fc/similar/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_8_0_8/_15, fc/similar/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'fc/similar/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 214, in mutation_spec_first
    similar_1 = single_fc_layer(similarity,15,15*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 15), b.shape=(15, 15), m=500, n=15, k=15
	 [[Node: fc/similar/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_8_0_8/_15, fc/similar/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:47:56.598133: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:56.719403: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:56.769609: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:56.893754: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:56.902271: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:56.967213: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.080520: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.091402: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:57.098831: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:57.141718: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.166388: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.210656: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:57.321538: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.355671: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.401242: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.436095: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.479716: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.484938: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_LAUNCH_FAILED
Time-21
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:57.496561: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.533290: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:57.537692: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.576541: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.694650: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.715621: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:57.724656: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.782268: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.859190: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:57.869189: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-51
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:57.920628: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.009146: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.101882: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.190766: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:58.194021: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.220564: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Mockito-23
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:58.247856: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:58.279444: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.287774: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:58.318238: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:58.344568: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.361593: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:58.392035: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:58.415527: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.534369: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:58.559029: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:58.579230: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.601273: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.605626: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Time-23
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:58.616966: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:58.643186: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:58.671625: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.690464: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.699346: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:58.701950: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_LAUNCH_FAILED
Mockito-8
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:58.719809: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:47:58.747740: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.769892: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.775518: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:58.846222: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.868937: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.946445: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:58.958609: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:59.048168: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:59.067707: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:59.134261: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:59.150813: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:59.200449: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:59.232205: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:59.283371: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:59.306232: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:59.354429: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:59.375928: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:59.413866: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:59.453976: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:59.489707: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:59.537597: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:59.581625: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:59.593490: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Mockito-26
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:59.616786: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:59.653706: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:59.702528: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:59.755669: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:59.809961: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:59.813181: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:59.828668: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287996416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:47:59.887072: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:59.900130: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:47:59.904102: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Mockito-28
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:47:59.955139: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Math-27
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut4/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_5_0_6/_9, seperate_mut/mut4/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_mut/mut4/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 196, in mutation_spec_first
    mut_4 = single_fc_layer(m4,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut4/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_5_0_6/_9, seperate_mut/mut4/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:47:59.987947: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.024304: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Mockito-3
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:00.028638: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.043885: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.094364: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.103303: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.115854: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.164738: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:00.168772: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.189127: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.202834: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.253626: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:00.256300: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.285402: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.304942: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.306721: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:00.335050: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:00.341442: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.359115: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.362482: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:00.364789: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.384705: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:00.404542: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.422367: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:00.425193: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.445318: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-13
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:00.448370: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:00.462454: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:00.585185: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:00.738926: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:00.762849: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:00.862037: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:00.963823: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:01.025287: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:01.085751: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:01.154040: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:01.214759: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:01.273888: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1287279104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:01.328718: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-53
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:01.621181: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:01.731201: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:01.760518: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:01.787032: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:01.817981: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:01.853744: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:01.854962: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Mockito-6
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:01.871363: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:01.924927: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:01.941483: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:01.984216: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:01.999190: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.240958: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.256506: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.291416: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.314363: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.352887: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.369533: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.376078: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.403831: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.411032: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.437095: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.443336: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-63
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:02.462478: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.467655: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.483485: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.512948: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.516747: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_LAUNCH_FAILED
Math-93
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:02.539098: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.542079: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.566584: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.578538: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.604540: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.606611: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.626371: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.632080: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.648273: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.650102: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Mockito-4
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:02.658101: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.679188: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.692749: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.735123: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.757694: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.790106: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.823371: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.825608: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-7
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:02.857417: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.898754: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.930659: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.949397: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:02.975480: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.009670: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.017654: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-16
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:03.031822: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.053474: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Mockito-2
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:03.059429: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.085037: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.114278: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.146746: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.166472: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.221022: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.223294: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.271163: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.274716: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.332967: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.335854: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:03.336243: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.496008: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.500421: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:03.503744: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.504027: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.556302: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.579023: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.583903: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:03.585452: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.612919: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.625493: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.629285: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.645355: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:03.647593: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.854714: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.867818: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.869314: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.872519: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:03.875591: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.907934: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.919968: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.923170: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:03.926772: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.968316: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.989880: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:03.991178: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:04.021870: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.032687: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:04.039835: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.098880: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.115442: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.143821: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:04.150581: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.170311: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.178002: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Chart-8
2019-01-18 12:48:04.359078: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:04.367621: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.373577: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.388813: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.394962: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.403183: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:04.426927: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.440209: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.449896: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.459938: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:04.460556: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Mockito-17
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:04.597641: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.602304: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.616411: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.634687: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.642616: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:04.656216: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.666664: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.682354: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.686341: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.691001: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:04.900239: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.905672: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.909746: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:04.952307: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:04.960720: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1288367104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:04.966909: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Time-24
2019-01-18 12:48:07.443982: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-43
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:07.859240: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-91
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:08.227180: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-58
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
Mockito-13
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 37), b.shape=(37, 37), m=500, n=37, k=37
	 [[Node: fc/complex/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_7_0_7/_13, fc/complex/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'fc/complex/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 211, in mutation_spec_first
    complex_1 = single_fc_layer(complexity,37,37*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 37), b.shape=(37, 37), m=500, n=37, k=37
	 [[Node: fc/complex/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_7_0_7/_13, fc/complex/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:48:08.373542: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-85
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:08.472878: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-112
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:09.139113: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-110
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:09.217923: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-118
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:09.306409: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-88
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:09.761329: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-97
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:09.776334: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-94
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:10.670884: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-34
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:10.852220: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-108
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:11.122385: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-103
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:11.387336: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-116
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:11.946863: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-114
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:12.005125: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-40
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:12.701477: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Closure-25
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:12.785534: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-22
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:13.179366: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.185097: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Closure-28
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:13.218531: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.313784: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.375179: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.380006: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Closure-106
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:13.431278: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.467856: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.496306: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.514288: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.538809: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.545979: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Closure-52
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:13.563558: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
2019-01-18 12:48:13.563806: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Closure-55
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:13.581294: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.591121: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.606789: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.620281: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Closure-49
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:13.626473: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.631170: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.661681: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.671945: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.672744: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.710255: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.731070: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.731881: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.759435: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.772120: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.773791: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.799402: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.800707: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Closure-61
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:13.806652: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.808304: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.830879: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Closure-31
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:13.849934: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.863018: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.878288: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.884141: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.906012: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.918659: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.920423: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.924470: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.942858: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.955922: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.959672: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.963571: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:13.989479: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.014328: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.022576: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.054841: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.062512: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.067785: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.107522: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.117195: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.126280: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.139176: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.142633: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.156694: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.191856: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.189549: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.224902: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.267295: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.268318: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.288924: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.312446: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.330445: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.330594: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.751348: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.766986: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.793732: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.794493: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:14.806716: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.251830: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.266133: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.266873: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.279197: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.290982: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.306750: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.312466: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.327865: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.364075: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.381548: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.387755: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.396864: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.420157: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.446954: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.449183: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.470584: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.471997: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Closure-64
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:15.497014: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.500253: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Closure-73
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:15.528354: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.531916: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.560136: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.612265: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.619142: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Closure-79
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:15.620616: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.632218: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.650063: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.665725: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.672366: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.681935: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.735734: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.748006: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.752715: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.755795: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.766153: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.786603: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.813877: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.823203: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.825045: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.827073: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.833847: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.861714: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.872236: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.874042: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.876190: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.883709: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.897340: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.914986: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.920365: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.921899: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.926992: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.947550: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.954908: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.969001: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.970130: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:15.990337: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.002144: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.007771: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.014733: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.016959: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.036414: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.053724: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.058915: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.071315: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.074181: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.082318: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.086846: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.096183: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.102939: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.104702: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.112143: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.116097: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.124347: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.132770: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.135456: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.158633: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.170613: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.180760: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.190521: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.194235: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.199024: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.204763: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.220293: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.224737: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.229404: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.238938: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.243717: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.248545: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.255714: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.262612: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.270278: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.276817: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.284284: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.293452: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.295716: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.296870: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.301543: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.306400: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.312107: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.320256: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.323024: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.328601: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.331940: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.335127: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.340439: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.356395: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.359171: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.404556: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.407987: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.411800: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.415510: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.436193: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.443497: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.450674: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.456167: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.463481: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.483139: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.487623: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.496874: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.501152: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.521613: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.524939: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.550332: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.555520: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.556918: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Closure-76
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:16.560275: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.564077: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.572902: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.576349: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.579359: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Closure-67
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:16.588368: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.590212: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.594953: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.596581: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.606621: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.610024: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.623092: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.624226: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.627602: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.629359: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.629505: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.636381: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.652376: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.654576: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.659216: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.660286: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.661033: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.675619: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.691574: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.693201: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.702765: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.704774: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.711064: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.717863: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.730521: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:16.743377: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.744459: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.751395: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.752180: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.756511: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:16.760224: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.765381: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.818172: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.819083: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.823230: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.824093: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.830034: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.836263: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:16.836694: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.852472: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:16.871370: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.875945: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.876891: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.877533: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.880095: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:16.882670: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:16.884060: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.908648: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.909843: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:16.914497: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:16.917535: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.924024: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.925606: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.928693: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.929440: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.941252: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:16.955835: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.959678: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:16.963190: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:16.964370: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.974098: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.974874: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.980217: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.981480: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:16.986249: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:16.990287: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.004987: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.008949: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.010654: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.013320: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.019702: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.021912: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.022006: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.023464: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.030866: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.031699: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.052282: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.054116: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.066469: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.069763: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.076403: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.078372: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.079827: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.087519: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.088778: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.095107: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.099312: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.102897: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.115136: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.120123: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.125545: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.126422: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.130712: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.137118: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.139486: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.141061: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.145843: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.150757: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.154658: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.156006: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.173541: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.176030: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.179541: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.184569: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.187395: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.189193: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.192209: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.192970: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.196581: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.198887: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.203810: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.206012: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.219754: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.223618: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.227163: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.230176: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.231554: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.242949: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.253693: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.254341: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.256091: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.264350: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.276756: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.277702: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.279305: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.290493: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.294933: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.296740: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.306114: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.307629: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.322956: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.324310: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.330417: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.331590: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.339962: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.340294: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.344639: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.348030: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.366994: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.371777: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.61M (1690112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.377509: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.379450: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.380568: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.387139: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Chart-19
2019-01-18 12:48:17.393129: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.402965: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.409260: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.411137: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.416015: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.417211: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.430412: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.434858: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.45M (1521152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.440707: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.446934: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.447727: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.448131: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.451137: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.460953: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.464270: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.466797: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.471532: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.478492: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.483452: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.488202: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.31M (1369088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.495488: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.497352: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.502832: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.506063: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.511328: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.516630: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.521142: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.523383: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.531566: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.536386: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.537955: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.544025: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.17M (1232384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.550741: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.553688: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.555681: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.556861: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.563907: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.569839: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.573390: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.576849: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.587425: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.588929: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.590518: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.600795: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.601022: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.06M (1109248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.604696: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.605901: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.608560: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.619979: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.968997: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.975388: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.979199: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.984768: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:17.992368: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.993847: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:17.998526: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.002941: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 975.0K (998400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.009202: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.011721: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.014342: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.015387: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.021277: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.027337: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.028721: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.037832: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.048372: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.050166: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.059113: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.062545: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.071013: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 877.5K (898560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.079663: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.082472: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.083838: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.084985: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.101359: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.106016: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.108180: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.123356: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.131217: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.132861: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.135963: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.147891: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.150727: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 789.8K (808704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.155345: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.157403: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.160339: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.165972: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.168238: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.176368: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.183858: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.187914: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.194444: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.196700: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.197732: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.200295: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 711.0K (728064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.201293: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.215516: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.219918: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.222361: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.230207: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.235968: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.243262: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.245848: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.249877: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.251768: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.263951: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.270417: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 640.0K (655360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.270714: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.272937: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.277080: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.282639: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.284430: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.291483: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.294829: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.316046: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.318923: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.323427: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.327021: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.330783: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.332811: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 576.0K (589824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.334842: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.339724: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.342636: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.343983: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.345486: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.353486: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.357420: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.364525: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.369750: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.373093: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.376916: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.378542: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 518.5K (530944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.379466: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.383591: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.384556: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.389528: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.393210: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.400935: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.441003: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.442875: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.450698: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.452214: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.460293: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.461919: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 466.8K (477952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.463232: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.470534: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.475541: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.476148: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.477799: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.480903: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.488636: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.491095: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.492943: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.61M (1690112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.499368: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.501662: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.520985: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.522529: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 420.2K (430336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.526820: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.538641: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.539433: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.544562: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.547275: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.556676: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.558458: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.45M (1521152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.558939: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.562493: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.571097: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.574626: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.580292: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 378.2K (387328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.581759: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.587885: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.596353: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.598502: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.598837: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.602730: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.604593: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Closure-82
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
2019-01-18 12:48:18.605479: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:18.609887: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.610880: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.31M (1369088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.617948: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.619545: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.621224: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.626758: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 340.5K (348672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.632880: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.640404: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.642832: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.644177: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Closure-70
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:18.652132: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.654971: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.656475: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.662849: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.17M (1232384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.664266: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.668694: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.674169: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.680852: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.683657: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.685149: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 306.5K (313856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.700534: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.707089: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.717409: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.722913: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.724329: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.733039: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.734857: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.06M (1109248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.737202: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.740953: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.748863: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.754394: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.756332: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 276.0K (282624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.759301: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.774623: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.780388: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.795234: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.797582: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.799523: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.801536: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.804365: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.806081: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 975.0K (998400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.810193: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.814186: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.819286: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.820603: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 248.5K (254464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.824092: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.844170: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.845213: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.851809: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.854259: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.859141: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.861322: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 877.5K (898560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.863271: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.867380: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.871152: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.872696: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 223.8K (229120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.875284: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.880632: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.877380: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.892123: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.903036: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.906558: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.912571: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.913178: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.916398: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 789.8K (808704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.921800: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.924137: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.926846: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.929866: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 201.5K (206336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.931991: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.939165: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.943351: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.959443: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.960367: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.960894: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.969043: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.969934: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.973192: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 711.0K (728064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.977057: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.980146: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 181.5K (185856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.981901: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:18.984905: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:18.987770: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.037192: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.045419: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.058359: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.061306: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.065367: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.61M (1690112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.069224: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.075741: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.077706: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 640.0K (655360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.088063: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.089779: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.090973: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.093641: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 163.5K (167424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.097769: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.109253: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.113351: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.122899: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.45M (1521152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.125764: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.127599: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.129691: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.130958: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.132343: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 576.0K (589824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.144038: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.144926: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.145206: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.147389: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 147.2K (150784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.151465: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.166936: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.174086: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.199934: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.201920: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.203622: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.204934: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.208044: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 518.5K (530944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.208184: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.31M (1369088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.216166: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.219695: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.222039: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 132.8K (135936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.226225: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.240135: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.245311: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.246865: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.259386: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.261071: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.262876: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.265054: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.266854: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 466.8K (477952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.269464: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.17M (1232384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.276655: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.279367: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.284264: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 119.5K (122368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.287035: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.300449: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.322651: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.323837: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.326509: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.327711: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.329500: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 420.2K (430336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.330820: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.06M (1109248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.335826: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.337452: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.340227: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.343191: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.344397: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 107.8K (110336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.354806: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.362142: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.369743: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.372222: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.373830: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 378.2K (387328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.378129: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 975.0K (998400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.379400: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.384713: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.386576: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.390013: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.390373: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.393129: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 97.0K (99328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.394742: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.406677: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.414945: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.418194: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.423798: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 340.5K (348672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.426652: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 877.5K (898560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.427945: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.434065: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.438266: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.440405: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.440836: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.448624: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 87.5K (89600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.449207: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.456714: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.469184: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.472066: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.477064: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 306.5K (313856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.485198: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.487258: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.487775: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 789.8K (808704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.489915: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.495452: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.496936: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.499164: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 78.8K (80640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.500714: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.500865: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.513154: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.514863: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.517189: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.524832: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 276.0K (282624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.530473: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.533920: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.535378: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 711.0K (728064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.539895: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.549940: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.552471: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.0K (72704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.552739: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.553638: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.556279: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.576044: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.579270: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.580871: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Closure-124
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 34), b.shape=(34, 34), m=500, n=34, k=34
	 [[Node: seperate_spec/spec/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_1_0_2/_11, seperate_spec/spec/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_spec/spec/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 204, in mutation_spec_first
    spec_1 = single_fc_layer(spec,34,34*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 34), b.shape=(34, 34), m=500, n=34, k=34
	 [[Node: seperate_spec/spec/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_1_0_2/_11, seperate_spec/spec/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:48:19.608903: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 248.5K (254464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.617045: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.618967: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.61M (1690112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.623712: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 640.0K (655360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.627561: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.632710: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.637227: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.0K (65536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.638548: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.639739: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.644513: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.658367: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.660715: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.672650: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 223.8K (229120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.678260: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.683757: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 576.0K (589824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.686925: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.691598: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.45M (1521152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.692809: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.8K (59136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.694073: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Closure-19
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError2019-01-18 12:48:19.700191: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.704600: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut3/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_4_0_5/_7, seperate_mut/mut3/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_mut/mut3/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 193, in mutation_spec_first
    mut_3 = single_fc_layer(m3,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut3/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_4_0_5/_7, seperate_mut/mut3/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:48:19.712662: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Closure-130
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError2019-01-18 12:48:19.716992: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_2_0_3/_1, seperate_mut/mut1/weight/read)]]

Caused by op u'seperate_mut/mut1/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 187, in mutation_spec_first
    mut_1 = single_fc_layer(m1,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut1/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_2_0_3/_1, seperate_mut/mut1/weight/read)]]

2019-01-18 12:48:19.720030: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.726188: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 201.5K (206336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.731571: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.745315: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.747432: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.0K (53248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.748883: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.31M (1369088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.751268: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.768833: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.775321: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.780130: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 181.5K (185856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.789515: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.793739: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 47.0K (48128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.802746: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.17M (1232384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.809253: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.822751: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.827435: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:19.836109: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 163.5K (167424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Closure-10
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError2019-01-18 12:48:19.842523: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Closure-1
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 15), b.shape=(15, 15), m=500, n=15, k=15
	 [[Node: fc/similar/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_8_0_8/_15, fc/similar/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'fc/similar/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 214, in mutation_spec_first
    similar_1 = single_fc_layer(similarity,15,15*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 15), b.shape=(15, 15), m=500, n=15, k=15
	 [[Node: fc/similar/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_8_0_8/_15, fc/similar/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut2/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_3_0_4/_5, seperate_mut/mut2/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_mut/mut2/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 190, in mutation_spec_first
    mut_2 = single_fc_layer(m2,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut2/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_3_0_4/_5, seperate_mut/mut2/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:48:19.857990: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.5K (43520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.859212: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.06M (1109248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.861098: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Closure-16
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 37), b.shape=(37, 37), m=500, n=37, k=37
	 [[Node: fc/complex/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_7_0_7/_13, fc/complex/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'fc/complex/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 211, in mutation_spec_first
    complex_1 = single_fc_layer(complexity,37,37*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 37), b.shape=(37, 37), m=500, n=37, k=37
	 [[Node: fc/complex/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_7_0_7/_13, fc/complex/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:48:19.872561: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.881280: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 147.2K (150784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.896853: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.2K (39168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.899278: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 975.0K (998400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.908001: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.916346: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 132.8K (135936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.934086: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 877.5K (898560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.934653: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.5K (35328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.966283: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.980476: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 119.5K (122368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.994174: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 789.8K (808704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:19.995286: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 31.2K (32000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.007665: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.019016: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 107.8K (110336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.031863: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 711.0K (728064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.033062: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 28.2K (28928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.070540: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.073804: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 97.0K (99328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.086208: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 640.0K (655360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.087422: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 25.5K (26112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.104937: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.107947: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 87.5K (89600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.116233: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 23.0K (23552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.118750: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 576.0K (589824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.119759: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Closure-37
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:20.147412: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.149648: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 78.8K (80640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.159332: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.8K (21248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.161687: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 518.5K (530944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.179129: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.0K (72704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.192488: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.198707: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.8K (19200 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.200617: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 466.8K (477952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.220840: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_LAUNCH_FAILED
Closure-126
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:20.230213: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.0K (65536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.245646: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.245776: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 17.0K (17408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.246863: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 420.2K (430336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.274138: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.8K (59136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.295831: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 15.5K (15872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.297558: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 378.2K (387328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.300922: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.310318: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.0K (53248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.327427: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.0K (14336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.329179: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 340.5K (348672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.341888: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.356422: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 12.8K (13056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.356969: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 47.0K (48128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.359422: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 306.5K (313856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.367860: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.425388: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.5K (11776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.427438: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 276.0K (282624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.429157: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.5K (43520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.434937: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.454816: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.5K (10752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.457624: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 248.5K (254464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.461895: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.2K (39168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.467691: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.639956: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 223.8K (229120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.645181: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.5K (35328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.660763: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.61M (1690112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.838741: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 31.2K (32000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.842034: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 201.5K (206336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.854502: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.45M (1521152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.865475: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 28.2K (28928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.868508: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 181.5K (185856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.899939: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.31M (1369088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.911997: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 25.5K (26112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.915051: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 163.5K (167424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:20.928660: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.17M (1232384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.211583: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 23.0K (23552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.212865: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 147.2K (150784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.230410: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.06M (1109248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.246229: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 132.8K (135936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.250440: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.8K (21248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.277097: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 975.0K (998400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.293934: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 119.5K (122368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.303374: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.8K (19200 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.328585: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 877.5K (898560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.338000: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 107.8K (110336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.383784: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 17.0K (17408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.398150: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 789.8K (808704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.417867: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 97.0K (99328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.423364: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 15.5K (15872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.434500: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 711.0K (728064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.444964: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 87.5K (89600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.452922: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.0K (14336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.460465: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 640.0K (655360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.465071: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 78.8K (80640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.493732: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 12.8K (13056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.496273: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 576.0K (589824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.501808: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.0K (72704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.773008: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.5K (11776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.775529: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 518.5K (530944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.787396: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.0K (65536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.814018: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 466.8K (477952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.815467: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.5K (10752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.828276: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.8K (59136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.849852: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 420.2K (430336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.854931: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.5K (9728 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.858641: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.0K (53248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.879136: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 378.2K (387328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.887283: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.8K (8960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.888830: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 47.0K (48128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.923235: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 340.5K (348672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.929329: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.0K (8192 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.931783: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.5K (43520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.955479: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 306.5K (313856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.962726: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.2K (7424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.964107: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.2K (39168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.988336: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 276.0K (282624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:21.993711: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.8K (6912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.002369: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.5K (35328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.022782: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 248.5K (254464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.025971: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.2K (6400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.035074: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 31.2K (32000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.044702: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 223.8K (229120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.054274: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.8K (5888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.091993: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 28.2K (28928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.106129: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 201.5K (206336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.120368: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.2K (5376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.134817: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 25.5K (26112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.152686: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 181.5K (185856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.166353: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.8K (4864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.187443: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 23.0K (23552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.210268: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 163.5K (167424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.217555: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.5K (4608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.243214: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.8K (21248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.260970: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 147.2K (150784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.264666: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.2K (4352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.282963: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.8K (19200 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.301567: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 132.8K (135936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.305854: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.0K (4096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.314497: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 17.0K (17408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.328488: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 119.5K (122368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.333528: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.8K (3840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.342322: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 15.5K (15872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.355845: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.5K (3584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.356803: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 107.8K (110336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.366224: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.0K (14336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.388831: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 97.0K (99328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.389586: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.2K (3328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.415360: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 12.8K (13056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.433276: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.0K (3072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.443237: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 87.5K (89600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.472465: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.5K (11776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.479971: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.8K (2816 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.488061: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 78.8K (80640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.496964: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.5K (10752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.505807: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.5K (2560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.514482: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.0K (72704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.526439: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.5K (9728 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.553412: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.567080: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.0K (65536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.574469: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.8K (8960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.582645: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.591185: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.8K (59136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.613289: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.0K (8192 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.617507: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.632789: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.0K (53248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.652484: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.2K (7424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.658152: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.675227: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 47.0K (48128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.700011: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.8K (6912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.705877: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.721194: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.5K (43520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.731398: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.2K (6400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.734077: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.735383: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289355776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.757647: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.8K (5888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Time-6
2019-01-18 12:48:22.763418: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.2K (39168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.765062: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289355776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.765816: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.792174: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.2K (5376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.797189: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.797397: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.5K (35328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.823086: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.8K (4864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.828487: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.837209: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 31.2K (32000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.868486: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.5K (4608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.873161: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.880733: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 28.2K (28928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.907034: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.2K (4352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.913783: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.917094: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 25.5K (26112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.936812: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.0K (4096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.972304: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.976664: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 23.0K (23552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.992303: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.8K (3840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:22.996240: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.003670: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.8K (21248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.021091: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.5K (3584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.023462: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.042463: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.8K (19200 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.058958: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.2K (3328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.061292: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.074251: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 17.0K (17408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.092499: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.0K (3072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.095255: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.112030: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 15.5K (15872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.123949: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.8K (2816 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.129847: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.143599: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.0K (14336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.154612: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.5K (2560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.158155: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.173168: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 12.8K (13056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.194756: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.205625: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.229669: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.5K (11776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.234031: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.267927: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.283739: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.5K (10752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.290060: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.300768: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.310923: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.5K (9728 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.327592: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.340825: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.350866: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.8K (8960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.367368: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.375160: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.386774: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.0K (8192 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.408781: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.414486: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.437449: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.2K (7424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.451898: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.466350: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.488624: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.8K (6912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.502800: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.504390: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.520365: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.2K (6400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.531130: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.542953: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.8K (5888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.543563: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.857325: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.872147: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.874603: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.2K (5376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.906274: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.910073: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.913198: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.8K (4864 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.929536: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.932302: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.940106: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.5K (4608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.957816: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.963628: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.976209: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.2K (4352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:23.999283: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.002432: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.034276: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.0K (4096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.048182: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.050936: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.072238: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.8K (3840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.076817: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.080454: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.116270: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.5K (3584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.119450: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.134514: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.141858: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.2K (3328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.166302: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.183293: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.192688: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Lang-2
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:24.194207: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.0K (3072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.199281: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.235134: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.250039: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.8K (2816 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.254102: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.270347: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.284467: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.5K (2560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.306710: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.319995: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Lang-8
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:24.307975: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.321668: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.352758: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.361297: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.361504: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.380262: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.384160: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.399217: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.416960: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.426167: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.427354: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.465816: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.475021: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.476301: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.530667: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.550384: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.550621: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.569194: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.588658: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.591542: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.609618: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.622757: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.625205: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.645084: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.659992: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.665091: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.676823: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.696724: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.705610: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.715469: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.753620: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.758287: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.768553: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.783432: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.795125: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.814087: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.829798: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.850094: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.857844: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.866514: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.890043: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.913943: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.920548: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.924961: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:24.945660: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.961862: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.963311: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.968365: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:24.975064: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.991801: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:24.995051: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.005720: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.010222: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-5
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:25.011874: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.028754: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.034405: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.043192: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.046285: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.056889: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.059868: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.068074: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.081817: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.086610: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.088350: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.099190: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.101782: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.122019: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.128457: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.135050: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.146949: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Lang-4
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:25.150087: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.178413: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.182181: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.190593: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.192015: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.206899: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.215903: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.224845: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.227916: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.246725: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.255743: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.256331: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.260930: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.262349: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.271250: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.287390: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.304534: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.306331: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.316214: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.318990: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.341419: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.354013: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.360309: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.373645: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:25.398917: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.703415: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.729499: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.735375: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.762192: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.771501: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.814835: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.816354: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:25.852270: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:26.115412: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:26.117433: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:26.145460: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Chart-24
Math-31
Time-26
2019-01-18 12:48:33.794168: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:33.845342: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:33.867615: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:33.944141: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:33.992101: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Time-20
Chart-15
Chart-18
2019-01-18 12:48:39.097386: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:48:39.180449: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:48:45.388654: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.447337: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.464425: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.530046: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.553738: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.613961: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.619884: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.631601: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.652138: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.668985: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.674403: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.684909: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.694112: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.701707: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.753744: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.760104: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.771154: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.792450: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.796561: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.835225: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.856827: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Lang-18
2019-01-18 12:48:45.916404: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.922838: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.986908: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:45.990302: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.023441: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.027813: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.068811: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.071732: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.116437: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.120540: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.127555: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Lang-14
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:46.155198: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.208566: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.210591: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.227041: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.279678: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.283852: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.307114: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Math-11
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:46.309955: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.331508: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.341856: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.356085: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.377449: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.380955: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.400666: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.404726: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.406756: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.415146: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Lang-16
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
2019-01-18 12:48:46.416026: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
Math-26
    Traceback (most recent call last):
with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "main.py", line 47, in <module>
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:46.424093: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.433027: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.446620: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.452608: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.455394: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.480835: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.484966: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.489065: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.495694: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:46.497569: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.505159: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.508582: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.526067: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.528441: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.564328: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.573782: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.585897: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.591335: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Lang-6
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:46.597838: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.607423: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.615943: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:46.616784: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.624023: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.628291: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.647148: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.648303: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:46.664673: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.674371: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.683313: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.697383: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.694577: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:46.700435: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:46.703848: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.707934: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.711284: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.726606: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.743459: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.747409: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.753953: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.754410: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:46.764026: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.767147: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:46.771588: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.776285: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.780014: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.787611: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.790692: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.793793: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:46.795658: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:46.800691: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.808176: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.825499: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.828883: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.853938: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.858911: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.870545: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.872284: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:46.874199: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.886260: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:46.886673: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.891345: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.914821: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.917525: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.944079: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:46.949425: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.960076: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:46.966024: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.970585: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.972069: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.985351: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:46.989799: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:46.996525: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.003373: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.007717: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.009623: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.018469: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.026369: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.029158: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.032035: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.034326: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.047112: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.054670: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.056258: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.065879: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.068539: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.086773: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.088449: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.101070: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.103076: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.103326: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.104967: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.112689: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.126601: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.128176: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.128589: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.129377: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.133874: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.140278: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.158033: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.159135: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.160220: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.162728: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.168162: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.189690: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.191317: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.192526: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.193926: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.225093: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.238926: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.256812: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.262893: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.269839: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.272644: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.275056: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.287964: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.300749: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.308150: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.310362: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.318807: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.322687: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.355898: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.381656: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.386852: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.387051: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.389425: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.392765: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.393531: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.417503: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.432203: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.432992: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.439507: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.449802: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.454300: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.480446: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.488017: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.488800: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.494018: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.494324: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.495706: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.507881: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.523327: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.524068: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.526654: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.529271: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.534350: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.540603: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.571656: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.572192: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.574901: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.584058: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.591264: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.602423: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.632540: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.633415: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.636489: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.642196: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.655460: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.674094: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.706659: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.707570: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.714389: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.715647: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.728995: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.736614: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.762998: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.766193: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.790119: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.792593: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.794264: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.801230: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.807859: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.816590: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.818835: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.837982: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.849656: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.857111: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.858216: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.868417: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.869061: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.875091: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.893577: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.899839: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.905366: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.908359: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.915241: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.915938: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.951533: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.958049: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.963990: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.974553: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.975000: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.975851: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:47.978590: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.983325: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:47.995466: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.005148: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.014249: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.019067: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.021030: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.022893: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.041288: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.044700: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.059834: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.068389: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.069800: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.081288: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.082544: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.084651: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.097063: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.117899: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.123730: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.135446: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.138450: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.141785: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.142583: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.144184: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.181015: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.195349: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.211805: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.214284: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.217257: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.222129: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.224044: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.231463: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.287195: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.308092: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.331192: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.333832: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.335623: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.337198: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.359210: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.366653: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.377218: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.386025: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.404479: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.409937: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.417431: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.419986: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.421474: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.424404: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.432202: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.440694: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.446396: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.475469: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.478502: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.481312: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.482819: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.507810: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.522556: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.529230: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.531527: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.547865: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.578226: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.580868: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.583149: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.583984: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.594576: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.598775: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.600262: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.602303: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.614903: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.621903: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.624871: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.627320: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.637526: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.645468: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.646603: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.662955: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.666350: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.666735: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.697365: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.699722: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.701638: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Lang-22
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut2/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_3_0_4/_5, seperate_mut/mut2/weight/read)]]
	 [[Node: seperate_mut/mut2/weight/Regularizer/l2_regularizer/_41 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1821_seperate_mut/mut2/weight/Regularizer/l2_regularizer", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_mut/mut2/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 190, in mutation_spec_first
    mut_2 = single_fc_layer(m2,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut2/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_3_0_4/_5, seperate_mut/mut2/weight/read)]]
	 [[Node: seperate_mut/mut2/weight/Regularizer/l2_regularizer/_41 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1821_seperate_mut/mut2/weight/Regularizer/l2_regularizer", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:48:48.712817: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.718423: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.722299: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.725294: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.727805: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.732365: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.737487: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.750644: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.763216: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.771125: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.774384: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.777179: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.777925: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.781836: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.782967: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Lang-30
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:48.789183: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.805313: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.821441: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.822289: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.828328: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.830836: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.837320: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.841408: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.844581: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.847489: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.857176: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.858895: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.866867: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.872090: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.875790: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.879511: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.881072: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.61M (1690112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.898461: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.906939: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.916153: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:48.917846: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.925082: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:48.928057: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.089867: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.45M (1521152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.092230: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.61M (1690112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.093663: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.094540: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.095791: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.097611: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.149995: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.31M (1369088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.157455: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.45M (1521152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.158858: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.160437: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.162420: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.168591: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Lang-26
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut3/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_4_0_5/_7, seperate_mut/mut3/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_mut/mut3/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 193, in mutation_spec_first
    mut_3 = single_fc_layer(m3,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut3/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_4_0_5/_7, seperate_mut/mut3/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:48:49.201708: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.201956: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.204164: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.204662: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.17M (1232384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.205293: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.31M (1369088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.211270: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-34
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:49.221254: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.236627: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.237981: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.240435: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.241695: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.06M (1109248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.244434: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.17M (1232384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.245589: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.277631: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.278881: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.281346: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.283308: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 975.0K (998400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.289531: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.06M (1109248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.290879: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.339222: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.339377: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.340369: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.343246: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.344954: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 877.5K (898560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.351094: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.351627: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 975.0K (998400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.371499: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.372936: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.375388: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.377384: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 789.8K (808704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.382438: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 877.5K (898560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.383035: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.414750: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.416129: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.417196: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.427294: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.432727: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 711.0K (728064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.435205: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.436301: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 789.8K (808704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.447357: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.458008: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.459640: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.61M (1690112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.465493: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.471185: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 640.0K (655360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.471378: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.484528: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 711.0K (728064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.489035: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.497002: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.500875: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.502157: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.45M (1521152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.505378: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.517097: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 576.0K (589824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.518610: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.537047: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 640.0K (655360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.538473: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.543125: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.546922: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.550194: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.31M (1369088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.554459: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.562711: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 518.5K (530944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.564783: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.574743: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 576.0K (589824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.576402: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.584101: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.586583: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.594915: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.17M (1232384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.595096: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.619597: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 466.8K (477952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.623202: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.643339: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 518.5K (530944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.644609: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.649130: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.61M (1690112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.657481: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.659099: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.06M (1109248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.659847: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.665642: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 420.2K (430336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.673684: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.685221: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 466.8K (477952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.686923: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.690891: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.45M (1521152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.703926: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 975.0K (998400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.704435: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.705813: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.709070: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 378.2K (387328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.755337: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 420.2K (430336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.760022: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.768854: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.31M (1369088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.794451: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 877.5K (898560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.795330: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.797248: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.801098: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.807971: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 340.5K (348672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.842480: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.17M (1232384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.842665: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 378.2K (387328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.844131: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.863556: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 789.8K (808704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.864465: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.880156: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 306.5K (313856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.886831: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.888738: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.907973: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.06M (1109248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.911479: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 340.5K (348672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.914461: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.920240: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.920513: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 711.0K (728064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.927751: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 276.0K (282624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.929783: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.935165: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.942055: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 975.0K (998400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.947779: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 306.5K (313856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.949215: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.958906: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.960841: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 640.0K (655360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.964649: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 248.5K (254464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.967872: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.974773: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:49.983330: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 877.5K (898560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.995428: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 276.0K (282624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:49.999312: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.000837: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.003133: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 576.0K (589824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.014133: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 223.8K (229120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.021659: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.026208: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.037133: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 789.8K (808704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.038335: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 248.5K (254464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.041018: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.043922: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.052240: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 518.5K (530944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.066067: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 201.5K (206336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.073840: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.075725: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.091910: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 711.0K (728064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.094070: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 223.8K (229120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.098048: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.099563: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.103800: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 466.8K (477952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.107329: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 181.5K (185856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.123409: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.127657: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.130846: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 640.0K (655360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.137789: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 201.5K (206336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.145329: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.146516: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.162427: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 420.2K (430336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.168237: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 163.5K (167424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.198482: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.200216: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 576.0K (589824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.204093: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.205148: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 181.5K (185856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.208754: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.210334: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.215299: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 378.2K (387328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.223115: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 147.2K (150784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.246284: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.248608: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 518.5K (530944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.254408: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.256311: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 163.5K (167424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.258090: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.259229: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.268146: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 340.5K (348672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.276032: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 132.8K (135936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.310473: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 466.8K (477952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.324143: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.326820: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.328903: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 147.2K (150784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.330279: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.332140: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.340734: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 306.5K (313856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Time-22
2019-01-18 12:48:50.360309: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 119.5K (122368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.421744: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 420.2K (430336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.431552: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.434318: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.435366: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 132.8K (135936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.436882: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.442796: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.446438: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 276.0K (282624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.456745: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 107.8K (110336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.481209: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 378.2K (387328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.487493: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.489825: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.491443: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 119.5K (122368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.491616: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.501754: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 248.5K (254464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.507038: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.508636: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 97.0K (99328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.549986: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 340.5K (348672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.556654: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.557621: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 107.8K (110336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.559261: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.559765: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.566315: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 223.8K (229120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.573508: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 87.5K (89600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.574916: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.602743: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 306.5K (313856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.605423: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 97.0K (99328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.608155: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.608541: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Lang-20
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut4/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_5_0_6/_9, seperate_mut/mut4/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_mut/mut4/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 196, in mutation_spec_first
    mut_4 = single_fc_layer(m4,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut4/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_5_0_6/_9, seperate_mut/mut4/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:48:50.631283: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 201.5K (206336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.635898: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 78.8K (80640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.637217: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.663026: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 276.0K (282624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.665762: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 87.5K (89600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.667424: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.674100: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 181.5K (185856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.678902: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.0K (72704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.680252: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.701509: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 248.5K (254464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.717155: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 78.8K (80640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.718931: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.719519: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Math-38
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:50.726608: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 163.5K (167424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.730819: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.0K (65536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.733059: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.746104: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 223.8K (229120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.776200: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.778809: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.0K (72704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.781668: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 147.2K (150784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.783743: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.8K (59136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.785459: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.792910: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 201.5K (206336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.811271: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.0K (65536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.812971: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 132.8K (135936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.814042: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.0K (53248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.816416: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Lang-12
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 34), b.shape=(34, 34), m=500, n=34, k=34
	 [[Node: seperate_spec/spec/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_1_0_2/_11, seperate_spec/spec/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_spec/spec/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 204, in mutation_spec_first
    spec_1 = single_fc_layer(spec,34,34*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 34), b.shape=(34, 34), m=500, n=34, k=34
	 [[Node: seperate_spec/spec/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_1_0_2/_11, seperate_spec/spec/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:48:50.831894: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 181.5K (185856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.835885: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.887591: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.8K (59136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.906589: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 119.5K (122368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.907697: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 47.0K (48128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.909733: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.929059: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 163.5K (167424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.935507: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.936995: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.963014: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.0K (53248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.967817: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 107.8K (110336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.968897: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.5K (43520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.992492: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 147.2K (150784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:50.997877: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:50.998552: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.000889: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Math-20
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 34), b.shape=(34, 34), m=500, n=34, k=34
	 [[Node: seperate_spec/spec/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_1_0_2/_11, seperate_spec/spec/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_spec/spec/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 204, in mutation_spec_first
    spec_1 = single_fc_layer(spec,34,34*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 34), b.shape=(34, 34), m=500, n=34, k=34
	 [[Node: seperate_spec/spec/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_1_0_2/_11, seperate_spec/spec/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:48:51.012839: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 47.0K (48128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.019188: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 97.0K (99328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.022624: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.2K (39168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.028735: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 132.8K (135936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.034437: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.050316: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.5K (43520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.051757: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 87.5K (89600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.054946: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.5K (35328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.074535: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.084466: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 119.5K (122368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.090441: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.123351: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.2K (39168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.124623: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 78.8K (80640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.127202: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 31.2K (32000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.148717: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 107.8K (110336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.153453: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.168732: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.5K (35328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.171953: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.0K (72704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.177386: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 28.2K (28928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.186797: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 97.0K (99328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.219955: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Chart-1
2019-01-18 12:48:51.227836: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 31.2K (32000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.229903: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.0K (65536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.231300: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 25.5K (26112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.251806: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 87.5K (89600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.264857: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.279588: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 28.2K (28928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.281921: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.8K (59136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.290082: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 23.0K (23552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.298423: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 78.8K (80640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.317352: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.322779: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.326782: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 25.5K (26112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.328423: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.0K (53248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.336328: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.8K (21248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.347122: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.0K (72704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.382378: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.384065: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 23.0K (23552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.385212: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 47.0K (48128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.395677: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.393289: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.8K (19200 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.398435: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.0K (65536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.414344: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Math-48
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:51.426954: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.8K (21248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.428223: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.5K (43520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.433579: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.435119: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.446524: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.8K (59136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.441881: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 17.0K (17408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.478161: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.8K (19200 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.487098: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.2K (39168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.493001: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.494979: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.0K (53248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.509512: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 15.5K (15872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.519799: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 17.0K (17408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.527737: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.5K (35328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.536506: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 47.0K (48128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.786408: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 31.2K (32000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.796674: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.852994: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 28.2K (28928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.890275: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.910619: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.919836: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 25.5K (26112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:51.946420: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.951459: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:51.958860: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 23.0K (23552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:52.111244: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.111999: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.114816: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.8K (21248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:52.152353: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.8K (19200 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:52.154407: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.171140: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.202090: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 17.0K (17408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:52.203796: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.239159: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.252078: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-42
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:52.253481: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.251273: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 15.5K (15872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:52.406007: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.427051: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.461955: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.476395: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.489395: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.505039: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.552844: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.565634: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.596828: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.695200: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.719955: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.733921: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.881383: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.891828: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.914552: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.941094: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:52.991311: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.000961: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.053231: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.062810: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.075748: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.079208: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.095728: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.133525: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.136343: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.144474: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.146363: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.177486: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289321984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.179715: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.182111: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.286555: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289349632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.289640: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.296059: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.309600: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289321984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.315697: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.321268: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.332794: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289349632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.333949: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.346290: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.362037: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.368821: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.376604: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.389903: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.390582: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.395632: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.411946: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.418799: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.420905: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.427475: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.434013: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.486846: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.488312: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.506988: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.512588: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.528367: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Chart-26
2019-01-18 12:48:53.533970: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289351168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.541696: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.544542: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.548982: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.568142: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.570188: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289351168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.582583: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Math-44
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError2019-01-18 12:48:53.584998: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.586166: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut3/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_4_0_5/_7, seperate_mut/mut3/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_mut/mut3/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 193, in mutation_spec_first
    mut_3 = single_fc_layer(m3,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut3/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_4_0_5/_7, seperate_mut/mut3/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:48:53.607456: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.617406: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.621763: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.641046: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.643126: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.664390: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.669209: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.677511: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.691979: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.695344: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.740018: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.745714: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.761255: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.776452: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.779206: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.791902: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.803444: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:53.811069: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.843883: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.853251: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.879993: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.889391: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:53.891358: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Math-46
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 34), b.shape=(34, 34), m=500, n=34, k=34
	 [[Node: seperate_spec/spec/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_1_0_2/_11, seperate_spec/spec/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_spec/spec/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 204, in mutation_spec_first
    spec_1 = single_fc_layer(spec,34,34*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 34), b.shape=(34, 34), m=500, n=34, k=34
	 [[Node: seperate_spec/spec/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_1_0_2/_11, seperate_spec/spec/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:48:53.908785: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.102895: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.118936: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.152496: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.174121: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.201383: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.219205: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.231798: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.259602: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.272326: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.300279: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.306313: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.330089: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.340181: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.354954: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.364652: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.395479: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.394956: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.415272: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.417492: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.451970: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.461140: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.495229: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.503414: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.522447: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.536652: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.561174: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.565414: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.589126: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.616053: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.629002: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.639467: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.666040: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory
Math-76
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:54.682131: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.686219: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.706598: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.738363: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.741060: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.750688: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 134.55M (141081856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.869780: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 121.09M (126973696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.881717: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.898864: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 108.98M (114276352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.924780: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.933926: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 98.08M (102848768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.944978: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:54.955122: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 88.28M (92563968 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:54.994298: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:55.011917: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 79.45M (83307776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.044824: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:55.064633: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.50M (74977024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.097516: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:55.097908: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.35M (67479552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.114026: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.92M (60731648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.136741: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Math-50
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError2019-01-18 12:48:55.157600: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.13M (54658560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
: Blas GEMM launch failed : a.shape=(500, 15), b.shape=(15, 15), m=500, n=15, k=15
	 [[Node: fc/similar/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_8_0_8/_15, fc/similar/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'fc/similar/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 214, in mutation_spec_first
    similar_1 = single_fc_layer(similarity,15,15*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 15), b.shape=(15, 15), m=500, n=15, k=15
	 [[Node: fc/similar/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_8_0_8/_15, fc/similar/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:48:55.319175: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 46.91M (49192704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.343363: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.22M (44273664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.367135: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.00M (39846400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.399174: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.20M (35861760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.439225: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 30.78M (32275712 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.485566: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 27.70M (29048320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.500463: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 24.93M (26143488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.558178: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 22.44M (23529216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.570623: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:55.578756: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.20M (21176320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.601385: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.18M (19058688 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.635874: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 16.36M (17153024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.658028: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:55.667481: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.72M (15437824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.828285: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:55.830007: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 13.25M (13894144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.878605: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.92M (12504832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.881421: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:55.922167: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 10.73M (11254528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:55.941712: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:55.997244: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 9.66M (10129152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:56.038415: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:56.071735: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 8.69M (9116416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:56.104826: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:56.132439: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.82M (8204800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:56.155350: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 7.04M (7384320 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:56.157876: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:56.202887: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 6.34M (6646016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:56.212403: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:56.217546: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.70M (5981440 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:56.361930: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 5.13M (5383424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:56.371467: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:56.404612: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.62M (4845312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:56.540650: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:56.733524: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 4.16M (4360960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:56.738172: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:56.770893: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.74M (3924992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:56.785271: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:56.789412: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.37M (3532544 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:56.825308: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:56.835364: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 3.03M (3179520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:56.868014: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.73M (2861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:56.869484: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:56.890162: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.46M (2575616 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:56.893002: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:56.903303: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 2.21M (2318080 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Lang-32
2019-01-18 12:48:57.020020: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.033302: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.99M (2086400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.048552: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.048650: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Math-14
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:57.080555: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.79M (1877760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.090880: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.094362: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.61M (1690112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.129099: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.45M (1521152 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.131480: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.178980: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.31M (1369088 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.186974: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.228089: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.17M (1232384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.233130: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.243052: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.06M (1109248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.265887: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 975.0K (998400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.267674: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.278865: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 877.5K (898560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.324751: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 789.8K (808704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.326197: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.397289: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 711.0K (728064 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.404388: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.433736: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 640.0K (655360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.441588: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.493993: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 576.0K (589824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.499750: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.505135: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 518.5K (530944 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.525611: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.536447: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 466.8K (477952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.589835: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 420.2K (430336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.605752: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.613063: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 378.2K (387328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.634349: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 340.5K (348672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.642198: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.663622: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 306.5K (313856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.682463: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.686135: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 276.0K (282624 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.716005: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 248.5K (254464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.760251: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.763543: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 223.8K (229120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.799669: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 201.5K (206336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.801279: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.823203: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 181.5K (185856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.844635: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.852754: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 163.5K (167424 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.890872: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:48:57.896413: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 147.2K (150784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.909974: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Math-74
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
2019-01-18 12:48:57.911602: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 132.8K (135936 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 34), b.shape=(34, 34), m=500, n=34, k=34
	 [[Node: seperate_spec/spec/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_1_0_2/_11, seperate_spec/spec/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_spec/spec/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 204, in mutation_spec_first
    spec_1 = single_fc_layer(spec,34,34*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 34), b.shape=(34, 34), m=500, n=34, k=34
	 [[Node: seperate_spec/spec/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_1_0_2/_11, seperate_spec/spec/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

2019-01-18 12:48:57.943001: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 119.5K (122368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.967079: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 107.8K (110336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:57.973000: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 97.0K (99328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.002708: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 87.5K (89600 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.015697: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 78.8K (80640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.036167: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_VALUE
Lang-10
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:48:58.039761: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 71.0K (72704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.062054: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 64.0K (65536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.109281: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 57.8K (59136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.185167: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 52.0K (53248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.201733: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 47.0K (48128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.336416: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 42.5K (43520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.358236: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 38.2K (39168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.405718: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 34.5K (35328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.419754: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 31.2K (32000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.441813: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 28.2K (28928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.475955: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 25.5K (26112 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.498308: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 23.0K (23552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.524656: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 20.8K (21248 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.730113: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 18.8K (19200 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.753577: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 17.0K (17408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.811311: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 15.5K (15872 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.856176: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 14.0K (14336 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:58.874720: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 12.8K (13056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:59.022653: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 11.5K (11776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:59.889017: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:59.926600: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:59.930559: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:59.933830: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:59.936481: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:59.949465: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:59.957944: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:59.968448: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 588.13M (616699904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:48:59.981302: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 529.32M (555030016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:00.007431: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 476.39M (499527168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:00.037887: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 428.75M (449574656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:00.054032: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 385.87M (404617216 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:00.056672: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 347.29M (364155648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:00.070895: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 312.56M (327740160 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:00.075515: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 281.30M (294966272 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:00.086208: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 253.17M (265469696 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:00.094757: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 227.85M (238922752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:00.105421: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 205.07M (215030528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:00.130626: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 184.56M (193527552 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:00.133492: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 166.11M (174174976 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:00.144176: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 149.50M (156757504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:00.321198: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289354752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:00.336067: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289354752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:01.869073: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:01.888039: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:01.901468: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:01.926155: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:01.940395: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:01.956534: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:01.982441: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:01.992527: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.030047: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.057768: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.107004: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.117160: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.141562: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.170339: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.183999: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.213742: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Time-1
2019-01-18 12:49:02.223533: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.227431: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.231309: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.248419: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.253136: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.282988: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.300078: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.352777: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.384670: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.414958: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.443948: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.450157: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.478096: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.487398: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.502194: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.542492: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.547865: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:02.593149: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Math-94
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut4/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_5_0_6/_9, seperate_mut/mut4/weight/read)]]

Caused by op u'seperate_mut/mut4/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 196, in mutation_spec_first
    mut_4 = single_fc_layer(m4,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut4/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_5_0_6/_9, seperate_mut/mut4/weight/read)]]

Chart-20
2019-01-18 12:49:02.759195: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:02.893480: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:02.901961: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:02.944083: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:02.961051: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:02.976651: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:02.992196: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:03.576228: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289351168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:03.579668: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289351168 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:03.579947: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Resource exhausted: OOM when allocating tensor of shape [84,128] and type float
	 [[Node: fc/fc1/weight/Adam/Initializer/zeros = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [84,128] values: [0 0 0]...>, _device="/job:localhost/replica:0/task:0/device:GPU:0"]()]]
Math-2
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 348, in run
    sess.run(init)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor of shape [84,128] and type float
	 [[Node: fc/fc1/weight/Adam/Initializer/zeros = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [84,128] values: [0 0 0]...>, _device="/job:localhost/replica:0/task:0/device:GPU:0"]()]]

Caused by op u'fc/fc1/weight/Adam/Initializer/zeros', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 340, in run
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost+regu_losses)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py", line 410, in minimize
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py", line 588, in apply_gradients
    self._create_slots(var_list)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adam.py", line 134, in _create_slots
    self._zeros_slot(v, "m", self._name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py", line 1134, in _zeros_slot
    new_slot_variable = slot_creator.create_zeros_slot(var, op_name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.py", line 181, in create_zeros_slot
    colocate_with_primary=colocate_with_primary)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.py", line 155, in create_slot_with_initializer
    dtype)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.py", line 65, in _create_slot_var
    validate_shape=validate_shape)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 1467, in get_variable
    aggregation=aggregation)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 1217, in get_variable
    aggregation=aggregation)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 527, in get_variable
    aggregation=aggregation)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 481, in _true_getter
    aggregation=aggregation)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 903, in _get_single_variable
    aggregation=aggregation)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 2443, in variable
    aggregation=aggregation)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 2425, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 2406, in default_variable_creator
    constraint=constraint)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py", line 259, in __init__
    constraint=constraint)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py", line 368, in _init_from_args
    initial_value(), name="initial_value", dtype=dtype)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 885, in <lambda>
    shape.as_list(), dtype=dtype, partition_info=partition_info)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py", line 100, in __call__
    return array_ops.zeros(shape, dtype)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py", line 1551, in zeros
    output = fill(shape, constant(zero, dtype=dtype), name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py", line 2794, in fill
    "Fill", dims=dims, value=value, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [84,128] and type float
	 [[Node: fc/fc1/weight/Adam/Initializer/zeros = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [84,128] values: [0 0 0]...>, _device="/job:localhost/replica:0/task:0/device:GPU:0"]()]]

2019-01-18 12:49:05.251905: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.288233: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.328963: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.335199: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.349935: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.359149: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.373268: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.381482: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.389697: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.396054: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.404576: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.420200: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.424279: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.431365: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.435301: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.440921: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.479089: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.483846: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.523928: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.552996: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.576666: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.581685: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.623395: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.644097: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.648872: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.654302: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.664021: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.667679: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.670997: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.674087: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.685863: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.698573: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.707961: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.767497: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.778643: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.782432: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.786683: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.791076: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.795327: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:49:05.800135: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Math-84
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 34), b.shape=(34, 34), m=500, n=34, k=34
	 [[Node: seperate_spec/spec/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_1_0_2/_11, seperate_spec/spec/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_spec/spec/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 204, in mutation_spec_first
    spec_1 = single_fc_layer(spec,34,34*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 34), b.shape=(34, 34), m=500, n=34, k=34
	 [[Node: seperate_spec/spec/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_1_0_2/_11, seperate_spec/spec/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Time-3
Time-14
2019-01-18 12:49:10.337396: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289354752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:10.338238: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289354752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:10.338406: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Resource exhausted: OOM when allocating tensor of shape [] and type float
	 [[Node: fc/complex/weight/Initializer/random_uniform/sub = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 0.569494784>, _device="/job:localhost/replica:0/task:0/device:GPU:0"]()]]
Math-68
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 348, in run
    sess.run(init)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor of shape [] and type float
	 [[Node: fc/complex/weight/Initializer/random_uniform/sub = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 0.569494784>, _device="/job:localhost/replica:0/task:0/device:GPU:0"]()]]

Caused by op u'fc/complex/weight/Initializer/random_uniform/sub', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 211, in mutation_spec_first
    complex_1 = single_fc_layer(complexity,37,37*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 27, in single_fc_layer
    weight = create_variables("weight",[input_dimension, output_dimension])
  File "/home/weili/deepfl_more_try/fc_based.py", line 22, in create_variables
    new_variables = tf.get_variable(name, shape=shape, initializer=initializer,regularizer=regularizer)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 1467, in get_variable
    aggregation=aggregation)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 1217, in get_variable
    aggregation=aggregation)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 527, in get_variable
    aggregation=aggregation)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 481, in _true_getter
    aggregation=aggregation)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 903, in _get_single_variable
    aggregation=aggregation)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 2443, in variable
    aggregation=aggregation)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 2425, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 2406, in default_variable_creator
    constraint=constraint)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py", line 259, in __init__
    constraint=constraint)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py", line 368, in _init_from_args
    initial_value(), name="initial_value", dtype=dtype)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py", line 885, in <lambda>
    shape.as_list(), dtype=dtype, partition_info=partition_info)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/initializers.py", line 145, in _initializer
    dtype, seed=seed)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/random_ops.py", line 243, in random_uniform
    return math_ops.add(rnd * (maxval - minval), minval, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 850, in binary_op_wrapper
    return func(x, y, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 8188, in sub
    "Sub", x=x, y=y, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [] and type float
	 [[Node: fc/complex/weight/Initializer/random_uniform/sub = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 0.569494784>, _device="/job:localhost/replica:0/task:0/device:GPU:0"]()]]

Math-77
Math-71
Math-81
Math-95
2019-01-18 12:49:18.262965: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:18.264302: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:18.278363: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:18.279919: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:49:18.281337: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
Mockito-24
Math-79
Lang-28
Lang-24
2019-01-18 12:49:23.317488: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-12
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
Math-89
2019-01-18 12:49:25.949423: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-15
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:49:27.957081: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-21
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
Math-55
Math-22
Math-35
2019-01-18 12:49:33.224111: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-3
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
Math-97
Math-65
Math-59
Math-101
Math-103
Math-39
Math-99
Mockito-30
Math-105
Math-69
Math-61
Mockito-7
Mockito-19
Mockito-20
Mockito-5
Math-91
Mockito-1
Mockito-36
2019-01-18 12:50:09.529340: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:50:09.656822: E tensorflow/core/common_runtime/direct_session.cc:158] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 11721506816
Math-106
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 347, in run
    with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1494, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 626, in __init__
    self._session = tf_session.TF_NewSession(self._graph._c_graph, opts)
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.
2019-01-18 12:50:10.184848: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.209294: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.217790: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.223398: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.253159: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.283622: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.305291: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.318105: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.323954: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.336549: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.347543: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.365587: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.381640: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.386575: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.394772: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.406910: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.410500: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.414775: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.422482: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.426799: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.431768: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.437833: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.440373: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.444321: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.449913: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.457961: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.461913: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.466201: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.470727: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.475389: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.480274: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.485219: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.492159: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.513992: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.519092: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.525929: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.530655: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.535065: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.541566: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2019-01-18 12:50:10.547693: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
Math-102
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 34), b.shape=(34, 34), m=500, n=34, k=34
	 [[Node: seperate_spec/spec/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_1_0_2/_11, seperate_spec/spec/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_spec/spec/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 204, in mutation_spec_first
    spec_1 = single_fc_layer(spec,34,34*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 34), b.shape=(34, 34), m=500, n=34, k=34
	 [[Node: seperate_spec/spec/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_1_0_2/_11, seperate_spec/spec/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Mockito-31
Mockito-11
Math-30
Math-86
Math-52
Math-58
Math-54
Math-88
Math-64
Math-8
Math-28
Math-17
Math-6
Math-32
Math-78
Math-72
Math-82
Math-90
Math-80
Math-36
Math-23
Math-56
Math-96
Math-60
Math-66
Math-40
Math-62
Math-98
Math-70
Math-92
Math-104
Math-100
Math-18
Math-9
2019-01-18 12:52:04.517301: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.20G (1289365504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:52:04.546920: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 1.08G (1160429056 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:52:04.549826: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 996.00M (1044386304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:52:04.578176: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 896.40M (939947776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:52:04.607915: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 806.76M (845953024 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:52:04.617329: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 726.09M (761357824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:52:04.645015: E tensorflow/stream_executor/cuda/cuda_driver.cc:903] failed to allocate 653.48M (685222144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY
2019-01-18 12:52:06.337234: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.342069: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.360121: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.363773: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.369096: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.384509: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.387933: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.397927: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.405641: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.426818: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.434470: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.441898: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.444296: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.459478: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.473640: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.478428: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.480947: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.506463: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.508305: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.510509: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.511970: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.549977: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.558332: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.563393: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.565770: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.570809: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.602320: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.608545: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.631117: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.646304: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.670613: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.673238: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.675733: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.698859: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.701762: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.707058: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.722504: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
2019-01-18 12:52:06.731677: E tensorflow/stream_executor/cuda/cuda_blas.cc:459] failed to create cublas handle: CUBLAS_STATUS_ALLOC_FAILED
Closure-41
Traceback (most recent call last):
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 365, in run
    y: batch_y, g: batch_g, keep_prob: dropout_rate,is_training:True})
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut2/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_3_0_4/_5, seperate_mut/mut2/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'seperate_mut/mut2/MatMul', defined at:
  File "main.py", line 47, in <module>
    main()
  File "main.py", line 41, in main
    fc.run(train_path,train_label_path, test_path,test_label_path, group_path ,susp_path, l, featureNum=feature,nodeNum=feature)
  File "/home/weili/deepfl_more_try/fc_based.py", line 328, in run
    pred = mutation_spec_first(spec, mutation1,mutation2,mutation3,mutation4,complexity,similarity, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 190, in mutation_spec_first
    mut_2 = single_fc_layer(m2,35,35*model_size_times, keep_prob,is_training)
  File "/home/weili/deepfl_more_try/fc_based.py", line 29, in single_fc_layer
    output_layer = tf.add(tf.matmul(input_layer, weight), bias)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py", line 2018, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py", line 4456, in mat_mul
    name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): Blas GEMM launch failed : a.shape=(500, 35), b.shape=(35, 35), m=500, n=35, k=35
	 [[Node: seperate_mut/mut2/MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device="/job:localhost/replica:0/task:0/device:GPU:0"](_arg_Placeholder_3_0_4/_5, seperate_mut/mut2/weight/read)]]
	 [[Node: Mean/_29 = _Recv[client_terminated=false, recv_device="/job:localhost/replica:0/task:0/device:CPU:0", send_device="/job:localhost/replica:0/task:0/device:GPU:0", send_device_incarnation=1, tensor_name="edge_1809_Mean", tensor_type=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:CPU:0"]()]]

Math-24
Closure-4
Closure-120
Closure-13
Closure-100
Closure-7
Closure-128
Closure-122
Closure-46
Closure-132
Closure-44
Closure-86
Closure-59
Closure-92
Closure-42
Closure-77
Closure-74
Closure-117
Closure-127
Closure-107
Closure-62
Closure-35
Closure-17
Closure-32
Closure-98
Closure-20
Closure-56
Closure-50
Closure-80
Closure-29
Closure-38
Closure-26
Closure-109
Closure-119
Closure-113
Closure-11
Closure-131
Closure-68
Closure-104
Closure-2
Closure-53
Closure-71
Closure-89
Closure-111
Closure-125
Closure-23
Closure-115
Closure-65
Closure-95
Closure-83
Closure-121
Closure-129
Closure-123
Closure-5
Closure-133
Closure-101
Closure-14
Closure-8
Closure-47
Closure-45
Closure-60
Closure-36
Closure-93
Closure-87
Closure-33
Closure-39
Closure-63
Closure-75
Closure-30
Closure-18
Closure-27
Closure-99
Closure-78
Closure-21
Closure-105
Closure-51
Closure-81
Closure-3
Closure-57
Closure-12
Closure-69
Closure-54
Closure-24
Closure-72
Closure-66
Closure-90
Closure-96
Closure-84
Closure-102
Closure-9
Closure-48
Closure-6
Closure-15
